# ðŸ” AI Model QA & Hallucination Detection Framework

[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/downloads/)
[![Pytest](https://img.shields.io/badge/tests-pytest-green.svg)](https://pytest.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://www.docker.com/)

An automated framework to **evaluate AI/LLM responses** for accuracy, hallucination, bias, and consistency using structured datasets and regression pipelines. Built for QA engineers, ML teams, and anyone deploying LLMs in production.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI Model QA Framework                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Datasets    â”‚â”€â”€â”€â–¶â”‚  Model Query â”‚â”€â”€â”€â–¶â”‚  Evaluation Engine   â”‚  â”‚
â”‚  â”‚   Manager     â”‚    â”‚   Engine     â”‚    â”‚                      â”‚  â”‚
â”‚  â”‚              â”‚    â”‚              â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚ â€¢ factual_qa â”‚    â”‚ â€¢ OpenAI API â”‚    â”‚  â”‚  Accuracy      â”‚  â”‚  â”‚
â”‚  â”‚ â€¢ consistencyâ”‚    â”‚ â€¢ HuggingFaceâ”‚    â”‚  â”‚  Hallucination â”‚  â”‚  â”‚
â”‚  â”‚ â€¢ hallucinateâ”‚    â”‚ â€¢ Retry/Back â”‚    â”‚  â”‚  Consistency   â”‚  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚  Bias Detectionâ”‚  â”‚  â”‚
â”‚                                          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                     â”‚              â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚
â”‚                      â–¼                              â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Regression Engine   â”‚    â”‚   Reporting System       â”‚          â”‚
â”‚  â”‚                      â”‚    â”‚                          â”‚          â”‚
â”‚  â”‚ â€¢ Baseline Manager   â”‚    â”‚  â€¢ CSV Reports           â”‚          â”‚
â”‚  â”‚ â€¢ Metric Comparison  â”‚    â”‚  â€¢ Matplotlib Charts     â”‚          â”‚
â”‚  â”‚ â€¢ Degradation Alert  â”‚    â”‚  â€¢ Dashboard PNG         â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  CI/CD: GitHub Actions  â”‚  Container: Docker & Compose     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
AI-Quality-Assurance-Framework/
â”œâ”€â”€ datasets/                    # Test datasets
â”‚   â”œâ”€â”€ factual_qa.json          # 50 factual Q&A pairs
â”‚   â”œâ”€â”€ consistency_prompts.json # 20 rephrased prompt sets
â”‚   â”œâ”€â”€ hallucination_prompts.json # 20 adversarial prompts
â”‚   â””â”€â”€ dataset_loader.py       # Dataset loading & validation
â”‚
â”œâ”€â”€ models/                      # Model adapters
â”‚   â”œâ”€â”€ base_model.py            # Abstract base with retry logic
â”‚   â”œâ”€â”€ openai_model.py          # OpenAI Chat Completions wrapper
â”‚   â”œâ”€â”€ huggingface_model.py     # HuggingFace Inference API wrapper
â”‚   â””â”€â”€ model_factory.py         # Factory pattern model creation
â”‚
â”œâ”€â”€ evaluation/                  # Evaluation engine
â”‚   â”œâ”€â”€ accuracy.py              # Exact, fuzzy, semantic matching
â”‚   â”œâ”€â”€ hallucination.py         # Multi-strategy hallucination detection
â”‚   â”œâ”€â”€ consistency.py           # Pairwise response consistency
â”‚   â”œâ”€â”€ bias_detector.py         # Sentiment, demographic, stereotype detection
â”‚   â””â”€â”€ metrics.py               # Aggregated metrics & composite scoring
â”‚
â”œâ”€â”€ regression/                  # Regression testing
â”‚   â”œâ”€â”€ baseline_manager.py      # Save/load metric baselines
â”‚   â””â”€â”€ regression_runner.py     # Compare & detect regressions
â”‚
â”œâ”€â”€ reports/                     # Reporting system
â”‚   â”œâ”€â”€ csv_reporter.py          # CSV output generation
â”‚   â”œâ”€â”€ visual_reporter.py       # Matplotlib chart generation
â”‚   â””â”€â”€ report_generator.py      # Report orchestrator
â”‚
â”œâ”€â”€ tests/                       # Pytest test suite
â”‚   â”œâ”€â”€ conftest.py              # Shared fixtures & mock data
â”‚   â”œâ”€â”€ test_dataset_loader.py   # Dataset tests (16 cases)
â”‚   â”œâ”€â”€ test_accuracy.py         # Accuracy tests (22 cases)
â”‚   â”œâ”€â”€ test_hallucination.py    # Hallucination tests (12 cases)
â”‚   â”œâ”€â”€ test_consistency.py      # Consistency tests (13 cases)
â”‚   â”œâ”€â”€ test_bias_detector.py    # Bias detection tests (8 cases)
â”‚   â”œâ”€â”€ test_regression.py       # Regression tests (10 cases)
â”‚   â””â”€â”€ test_reporters.py        # Reporter tests (13 cases)
â”‚
â”œâ”€â”€ scripts/                     # CLI entry points
â”‚   â”œâ”€â”€ run_evaluation.py        # Full evaluation pipeline
â”‚   â””â”€â”€ run_regression.py        # Regression test runner
â”‚
â”œâ”€â”€ .github/workflows/           # CI/CD
â”‚   â”œâ”€â”€ ci.yml                   # Test on push/PR
â”‚   â””â”€â”€ evaluation.yml           # Scheduled evaluation
â”‚
â”œâ”€â”€ config.yaml                  # Configuration file
â”œâ”€â”€ .env.example                 # API key template
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ Dockerfile                   # Container build
â”œâ”€â”€ docker-compose.yml           # Container orchestration
â””â”€â”€ README.md                    # This file
```

---

## Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/your-username/ai-qa-framework.git
cd ai-qa-framework

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Keys

```bash
# Copy the template
cp .env.example .env

# Edit .env with your keys
# OPENAI_API_KEY=sk-your-key-here
# HUGGINGFACE_API_TOKEN=hf_your-token-here
```

### 3. Run Tests (No API Keys Needed)

```bash
# Run the full test suite
python -m pytest tests/ -v

# With coverage
python -m pytest tests/ -v --cov=evaluation --cov=datasets --cov-report=term-missing
```

### 4. Run Evaluation (Requires API Key)

```bash
# Full evaluation pipeline
python scripts/run_evaluation.py

# Specific model provider
python scripts/run_evaluation.py --model openai
python scripts/run_evaluation.py --model huggingface

# Custom dataset
python scripts/run_evaluation.py --dataset datasets/factual_qa.json
```

### 5. Run Regression Test

```bash
python scripts/run_regression.py
```

---

## Docker

```bash
# Build the image
docker build -t ai-qa-framework .

# Run evaluation
docker run --env-file .env ai-qa-framework

# Run tests
docker run ai-qa-framework python -m pytest tests/ -v

# Docker Compose
docker compose up evaluator
docker compose --profile test up test
```

---

## Evaluation Metrics

| Dimension | Metrics | Weight |
|-----------|---------|--------|
| **Accuracy** | Exact Match Rate, Fuzzy Score, Semantic Similarity | 35% |
| **Hallucination** | Contradiction, Fabrication, Hallucination Rate | 30% |
| **Consistency** | Pairwise Similarity, Contradiction Detection | 20% |
| **Bias** | Sentiment Skew, Demographic Balance, Stereotypes | 15% |

### Overall Score Formula

```
Overall = (Accuracy Ã— 0.35) + ((100 - HallucinationRate) Ã— 0.30) +
          (Consistency Ã— 0.20) + ((100 - BiasRate) Ã— 0.15)
```

---

## Generated Reports

After running an evaluation, reports are saved to the `reports/` directory:

| Type | File | Description |
|------|------|-------------|
| CSV | `reports/csv/summary_*.csv` | All metrics in tabular format |
| CSV | `reports/csv/accuracy_detail_*.csv` | Per-question accuracy scores |
| CSV | `reports/csv/hallucination_detail_*.csv` | Per-prompt hallucination flags |
| PNG | `reports/charts/accuracy_metrics.png` | Accuracy bar & pie charts |
| PNG | `reports/charts/hallucination_metrics.png` | Hallucination rate visualization |
| PNG | `reports/charts/consistency_metrics.png` | Consistency score distribution |
| PNG | `reports/charts/regression_comparison.png` | Baseline vs current comparison |
| PNG | `reports/charts/overall_dashboard.png` | Combined metrics dashboard |

---

## Configuration

Edit `config.yaml` to customize:

```yaml
model:
  provider: "openai"        # or "huggingface"
  openai:
    model_name: "gpt-3.5-turbo"
    temperature: 0.0

evaluation:
  accuracy:
    fuzzy_match_threshold: 0.80
  hallucination:
    max_acceptable_rate: 0.10

regression:
  degradation_threshold: 0.05  # Flag if metric drops >5%
```

---

## CI/CD Pipeline

### Automatic Testing (`ci.yml`)
- **Trigger**: Push/PR to `main`
- **Matrix**: Python 3.10, 3.11, 3.12
- **Steps**: Install â†’ Lint â†’ Test â†’ Upload Coverage

### Scheduled Evaluation (`evaluation.yml`)
- **Trigger**: Weekly + manual dispatch
- **Steps**: Install â†’ Evaluate â†’ Regression Check â†’ Upload Reports

### Setting Up Secrets
Add your API keys to GitHub repository secrets:
- `OPENAI_API_KEY`
- `HUGGINGFACE_API_TOKEN`

---


## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

---

## Roadmap

- [ ] Multi-model comparison dashboards
- [ ] Fine-tuned dataset evaluation
- [ ] Human feedback loop integration
- [ ] Slack/email alerts for regressions
- [ ] Web-based dashboard UI
- [ ] Custom metric plugin system
